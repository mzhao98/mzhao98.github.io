<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Markov Decision Processes</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            overflow: hidden;
        }
        .code-snippet {
            background-color: #f4f4f4;
            border-left: 3px solid #f36d33;
            color: #666;
            page-break-inside: avoid;
            font-family: 'Courier New', Courier, monospace;
            padding: 15px;
            margin: 15px 0;
            overflow: auto;
        }
        .text-description {
            margin: 15px 0;
        }
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }
        #github-link {
            background-color: #2c3e50;
            color: white;
            padding: 20px 0;
            text-align: center;
        }
        #github-link a {
            color: #f39c12;
            font-size: 20px;
            text-decoration: none;
            font-weight: bold;
        }
        #github-link a:hover {
            color: #d35400;
            text-decoration: underline;
        }
        #modules ul {
            list-style-type: none;
            padding: 0;
        }
        #modules ul li {
            margin-bottom: 10px;
            position: relative;
            padding-left: 20px;
            line-height: 1.6;
        }
        #modules ul li:before {
            content: '\2022'; /* Unicode for a bullet point */
            color: #e8491d; /* Change this color to suit your design */
            font-size: 20px; /* Size of the bullet point */
            position: absolute;
            left: 0;
            top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Partially Observable Markov Decision Processes</h1>

        <!-- Section 1 -->
        <div class="text-description">
            <h2>A Basic Introduction</h2>
            <p>
                In this tutorial, we will be exploring the basics of Partially Observable Markov Decision Processes (POMDPs).

            </p>

        </div>


        <div class="text-description">
            <p>
               Define alpha vector class.
            </p>
        </div>
        <div class="code-snippet">
            <pre><code>
    MIN = -np.inf

    class AlphaVector(object):
        """
        Simple wrapper for the alpha vector used for representing the value function for a POMDP as a piecewise-linear,
        convex function
        """
        def __init__(self, a, v):
            self.action = a
            self.v = v

        def copy(self):
                return AlphaVector(self.action, self.v)
            </code></pre>
        </div>

        <div class="code-snippet">
            <pre><code>

    discount = 0.9

    human_prefers_pans, human_prefers_plates = 0,1
    robot_takes_pan, robot_takes_plate = 0,1
    human_takes_pan, human_takes_plate = 0,1

    states = [human_prefers_pans, human_prefers_plates]
    actions = [robot_takes_pan, robot_takes_plate]
    observations = [human_takes_pan, human_takes_plate]

    states_text = {human_prefers_pans: 'human_prefers_pans', human_prefers_plates: 'human_prefers_plates'}
    actions_text = {robot_takes_pan: 'robot_takes_pan', robot_takes_plate: 'robot_takes_plate'}
    observations_text = {human_takes_pan: 'human_takes_pan', human_takes_plate: 'human_takes_plate'}

    num_states = 2
    num_actions = 2
    num_observations = 2

    T = np.array([[[1.0,0.0] , [0.0,1.0] ],
        [[1.0,0.0] , [0.0,1.0] ]])

    O = np.array([[[0.85, 0.15] , [0.15, 0.85] ],
        [[0.85, 0.15] , [0.15, 0.85] ]])


    R = np.array([[-100.0, 10.0 ],
        [10.0, -100.0 ]])

    C = [-1, -1]
            </code></pre>
        </div>

        <div class="code-snippet">
            <pre><code>


    def solve_pomdp(max_timesteps, alpha_vecs, gamma_reward):

        for step in range(max_timesteps):

            # First compute a set of updated vectors for every action/observation pair
            gamma_intermediate = {}
            for a in actions:
                gamma_intermediate[a] = {}
                for o in observations:

                    gamma_action_obs = []
                    for alpha in alpha_vecs:
                        v = np.zeros(num_states)  # initialize the update vector [0, ... 0]
                        for i, si in enumerate(states):
                            for j, sj in enumerate(states):
                                v[i] += T[a, si, sj] * O[a, sj, o] * alpha.v[j]
                            v[i] *= discount
                        gamma_action_obs.append(v)

                    gamma_intermediate[a][o] = gamma_action_obs

            # Now compute the cross sum
            gamma_action_belief = {}
            for a in actions:

                gamma_action_belief[a] = {}
                for bidx, b in enumerate(belief_points):

                    gamma_action_belief[a][bidx] = gamma_reward[a].copy()

                    for o in observations:
                        # only consider the best point
                        best_alpha_idx = np.argmax(np.dot(gamma_intermediate[a][o], b))
                        gamma_action_belief[a][bidx] += gamma_intermediate[a][o][best_alpha_idx]


            # Finally compute the new(best) alpha vector set
            alpha_vecs, max_val = [], MIN

            for bidx, b in enumerate(belief_points):
                best_av, best_aa = None, None

                for a in actions:
                    val = np.dot(gamma_action_belief[a][bidx], b)
                    if best_av is None or val > max_val:
                        max_val = val
                        best_av = gamma_action_belief[a][bidx].copy()
                        best_aa = a

                alpha_vecs.append(AlphaVector(a=best_aa, v=best_av))


        return alpha_vecs
            </code></pre>
        </div>

        <div class="code-snippet">
            <pre><code>
                # setup alpha vectors
    alpha_vecs = [AlphaVector(a=-1, v=np.zeros(num_states))]
    stepsize = 0.1
    belief_points = [[np.random.uniform() for s in states] for p in np.arange(0., 1. + stepsize, stepsize)]


    gamma_reward = {
                a: np.array( [R[a,s] for s in states])
                for a in actions
            }

    max_timesteps = 10



    rand_nums = np.random.randint(0, 100, size=num_states)
    base = sum(rand_nums)*1.0
    belief = [x/base for x in rand_nums]
    belief = [0.5, 0.5]
    state = 1



    total_rewards = 0
    budget = 20
    max_play = 20
    for i in range(max_play):
        # plan, take action and receive environment feedbacks
        alpha_vecs = solve_pomdp(max_timesteps, alpha_vecs, gamma_reward)
        print("belief", belief)

        for av in alpha_vecs:
            v = np.dot(av.v, belief)


        max_v = -np.inf
        best = None
        for av in alpha_vecs:
            v = np.dot(av.v, belief)
            if v > max_v:
                max_v = v
                best = av
        action = best.action

        print("state ", states_text[state])
        print("action ", actions_text[action])


        ai = action
        si = state
        # get new state
        s_probs = [T[ai, si, sj] for sj in states]
        next_state = states[np.random.choice(np.arange(num_states), p=s_probs)]

        # get new observation
        o_probs = [O[ai, next_state, oj] for oj in observations]
        next_obs = observations[np.random.choice(np.arange(num_observations), p=o_probs)]

        next_reward = R[ai, si]
        next_cost = C[ai]

        print("reward ", next_reward)
        print("next_obs ", observations_text[next_obs])
        print("next_state ", states_text[next_state])
        print()

        new_state, obs, reward, cost = next_state, next_obs, next_reward, next_cost


        b_new = []
        for sj in states:
            p_o_prime = O[action, sj, obs]
            summation = 0.0
            for i, si in enumerate(states):
                p_s_prime = T[action, si, sj]
                summation += p_s_prime * float(belief[i])
            b_new.append(p_o_prime * summation)

        # normalize
        total = sum(b_new)
        belief = [x / total for x in b_new]



        total_rewards += reward
        budget -= cost

        if budget <= 0:
            print('Budget spent.')
            break

            </code></pre>
        </div>

        <!-- Repeat the pattern for additional sections -->

    </div>
</body>
</html>
