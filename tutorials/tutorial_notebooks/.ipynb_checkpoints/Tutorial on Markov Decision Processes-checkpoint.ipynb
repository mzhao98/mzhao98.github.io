{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### An Intuitive Introduction\n",
    "\n",
    "A Markov decision process (MDP) is a stochastic, discrete-time control process. In reinforcement learning (RL), an MDP serves as a mathematical framework for modeling the decision-making of an agent in some environment. Intuitively and perhaps oversimply, we can think of MDPs as modeling some decision-maker (agent), who has a problem to solve (operating properly in some problem-solving environment). Defining an MDP sets up the scenario: agent with the problem to solve. And solving an MDP entails coming up with the \"correct\" set of actions the agent must take to solve the problem. We'll next discuss the MDP definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Definition\n",
    "\n",
    "An MDP is defined by a tuple of variables $\\langle S,A,T,R,\\gamma,P_0 \\rangle$. \n",
    "- $S$ represents the set of states. In other words, this is all of the possible forms the agent's environment could take.\n",
    "- $A$ represents the set of actions available for the agent to take.\n",
    "- $P_0: S \\rightarrow \\mathcal{R}$ represents the initial state distribution. $P_0(s)$ for any state $s \\in S$ represents how likely the agent's environment is to start in state $s$.\n",
    "- $T: S \\times A \\times S \\rightarrow \\mathcal{R}$ is the transition function. $T(s'|s,a)$ gives the probability that the environment transitions to state $s'$ from state $s$ after taking action $a$. \n",
    "- $R: S \\times A \\rightarrow \\mathcal{R}$ represents the agent's objective. The agent's objective is embedded into the environment through the reward. $R(s,a)$ returns a real valued scalar representing how much reward the agent receives for taking action $a$ in state $s$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Definition: Toy Example\n",
    "\n",
    "The MDP definition is a bit abstract. Let's ground the components of an MDP in a very simple toy example.\n",
    "\n",
    "In our toy example, Alice (our agent) has a problem she needs to solve. The problem is that she dropped both a dirty shirt on the ground in the middle of her room. She needs to quickly and efficiently pick up the dirty shirt and place it in the laundry basket.\n",
    "\n",
    "We'll represent her room as a 5 by 5 grid. On the upper right side, she has a laundry basket. On the lower right side, she has a dresser. Alice is starting at the lower right side, by the door. The dirty shirt is in the middle of the room.\n",
    "\n",
    "An MDP is defined by a tuple of variables $\\langle S,A,T,R,\\gamma,P_0 \\rangle$. \n",
    "- $S$ represents the set of states. These are all of the positions that Alice can be in. All of the coordinate positions in the 5x5 room.\n",
    "- $A$: Alice can [move up, down, left, right, pick up shirt, place shirt].\n",
    "- $P_0$: With probability 1, Alice will begin in the lower right corner. This is deterministic.\n",
    "- $T$: This is also deterministic. All of Alice's action will succeed, if the action she is trying to perform is valid.\n",
    "- $R$: Alice will receive position reward if she places the shirt in the laundry bin. She will receive negative reward if she places the shirt in the dresser, as this will stink up the rest of her clothes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make this even more concrete, and get to coding up this toy example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to import helpful libraries: Numpy for computation and Matplotlib for visualization of states in the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a few global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_SHIRT = 'dirty_shirt'\n",
    "PICKUP = 'pickup'\n",
    "PLACE = 'place'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gridworld():\n",
    "    def __init__(self, initial_config, reward_dict):\n",
    "        self.reward_dict = reward_dict\n",
    "        self.initial_config = initial_config\n",
    "\n",
    "        self.target_object = reward_dict['target_object']\n",
    "        self.target_goal = reward_dict['target_goal']\n",
    "\n",
    "        self.square_positions = initial_config['square_positions']\n",
    "        self.triangle_positions = initial_config['triangle_positions']\n",
    "        self.start_pos = initial_config['start_pos']\n",
    "        self.g1 = initial_config['g1']\n",
    "        self.g2 = initial_config['g2']\n",
    "        self.objects_in_g1 = None\n",
    "        self.objects_in_g2 = None\n",
    "\n",
    "        self.set_env_limits()\n",
    "\n",
    "        self.directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "\n",
    "        # get possible joint actions and actions\n",
    "        self.possible_single_actions = self.make_actions_list()\n",
    "        # print(\"possible single actions\", self.possible_single_actions)\n",
    "\n",
    "\n",
    "        self.current_state = self.create_initial_state()\n",
    "        # self.reset()\n",
    "\n",
    "\n",
    "        # set value iteration components\n",
    "        self.transitions, self.rewards, self.state_to_idx, self.idx_to_action, \\\n",
    "        self.idx_to_state, self.action_to_idx = None, None, None, None, None, None\n",
    "        self.vf = None\n",
    "        self.pi = None\n",
    "        self.policy = None\n",
    "        self.epsilson = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.maxiter = 10000\n",
    "\n",
    "        # self.step_cost = -0.01\n",
    "        # self.push_switch_cost = -0.05\n",
    "\n",
    "        # self.step_cost = reward_weights[-2]\n",
    "        # self.push_switch_cost = reward_weights[-1]\n",
    "\n",
    "        self.num_features = 4\n",
    "        self.correct_target_reward = 10\n",
    "\n",
    "\n",
    "\n",
    "    def make_actions_list(self):\n",
    "        actions_list = []\n",
    "        actions_list.extend(self.directions)\n",
    "        actions_list.append(PICKUP)\n",
    "        actions_list.append(PLACE)\n",
    "        return actions_list\n",
    "\n",
    "\n",
    "    def set_env_limits(self):\n",
    "        # set environment limits\n",
    "        self.x_min = 0\n",
    "        self.x_max = 5\n",
    "        self.y_min = 0\n",
    "        self.y_max = 5\n",
    "\n",
    "        self.all_coordinate_locations = list(product(range(self.x_min,self.x_max),\n",
    "                                                     range(self.y_min, self.y_max)))\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.create_initial_state()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_initial_state(self):\n",
    "        # create dictionary of object location to object type and picked up state\n",
    "        state = {}\n",
    "        state['pos'] = copy.deepcopy(self.start_pos)\n",
    "        # state['square_positions'] = copy.deepcopy(self.square_positions)\n",
    "        # state['triangle_positions'] = copy.deepcopy(self.triangle_positions)\n",
    "        state['holding'] = None\n",
    "        state['objects_in_g1'] = None\n",
    "        state['objects_in_g2'] = None\n",
    "        # state['g1'] = copy.deepcopy(self.g1)\n",
    "        # state['g2'] = copy.deepcopy(self.g2)\n",
    "        # state['target_object'] = self.target_object\n",
    "        # state['target_goal'] = self.target_goal\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def is_done_given_state(self, current_state):\n",
    "        # check if player at exit location\n",
    "        # print(\"current state\", current_state)\n",
    "        if current_state['objects_in_g1'] != None or current_state['objects_in_g2'] != None:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def is_valid_push(self, current_state, action):\n",
    "        # action_type_moved = current_state['currently_pushing']\n",
    "        # if action_type_moved is None:\n",
    "        #     return False\n",
    "        # print(\"action type moved\", action_type_moved)\n",
    "        current_loc = current_state['pos']\n",
    "\n",
    "        new_loc = tuple(np.array(current_loc) + np.array(action))\n",
    "        if new_loc[0] < self.x_min or new_loc[0] >= self.x_max or new_loc[1] < self.y_min or new_loc[1] >= self.y_max:\n",
    "            return False\n",
    "\n",
    "        # if new_loc in current_state['grid'].values() and new_loc != current_loc:\n",
    "        #     return False\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def step_given_state(self, input_state, action):\n",
    "        step_cost = -0.1\n",
    "        current_state = copy.deepcopy(input_state)\n",
    "\n",
    "        # print(\"action\", action)\n",
    "        # check if action is exit\n",
    "        # print(\"action in step\", action)\n",
    "\n",
    "        if self.is_done_given_state(current_state):\n",
    "            step_reward = 0\n",
    "            return current_state, step_reward, True\n",
    "\n",
    "\n",
    "        if action in self.directions:\n",
    "            if self.is_valid_push(current_state, action) is False:\n",
    "                step_reward = step_cost\n",
    "                return current_state, step_reward, False\n",
    "\n",
    "        if action == PICKUP:\n",
    "\n",
    "            # if not holding anything\n",
    "            if current_state['holding'] is None:\n",
    "                # check if there is an object to pick up\n",
    "                if current_state['pos'] in self.square_positions:\n",
    "                    current_state['holding'] = 'square'\n",
    "                elif current_state['pos'] in self.triangle_positions:\n",
    "                    current_state['holding'] = 'triangle'\n",
    "\n",
    "                step_reward = step_cost\n",
    "                return current_state, step_reward, False\n",
    "\n",
    "            else:\n",
    "                step_reward = step_cost\n",
    "                return current_state, step_reward, False\n",
    "\n",
    "        if action == PLACE:\n",
    "            if current_state['holding'] is not None:\n",
    "                holding_object = current_state['holding']\n",
    "                if current_state['pos'] == self.g1:\n",
    "                    current_state['objects_in_g1'] = current_state['holding']\n",
    "                    current_state['holding'] = None\n",
    "                    step_reward = step_cost\n",
    "                    done = self.is_done_given_state(current_state)\n",
    "                    if self.target_object == holding_object and self.target_goal == 'g1':\n",
    "                        step_reward += self.correct_target_reward\n",
    "                        return current_state, step_reward, done\n",
    "                elif current_state['pos'] == self.g2:\n",
    "                    current_state['objects_in_g2'] = current_state['holding']\n",
    "                    current_state['holding'] = None\n",
    "                    step_reward = step_cost\n",
    "                    done = self.is_done_given_state(current_state)\n",
    "                    if self.target_object == holding_object and self.target_goal == 'g2':\n",
    "                        step_reward += self.correct_target_reward\n",
    "                        return current_state, step_reward, done\n",
    "\n",
    "                step_reward = step_cost\n",
    "                return current_state, step_reward, False\n",
    "            else:\n",
    "                step_reward = step_cost\n",
    "                return current_state, step_reward, False\n",
    "\n",
    "\n",
    "        current_loc = current_state['pos']\n",
    "        # print(\"current loc\", current_loc)\n",
    "        # print(\"action\", action)\n",
    "        new_loc = tuple(np.array(current_loc) + np.array(action))\n",
    "        current_state['pos'] = new_loc\n",
    "        step_reward = step_cost\n",
    "        done = self.is_done_given_state(current_state)\n",
    "\n",
    "\n",
    "        return current_state, step_reward, done\n",
    "\n",
    "\n",
    "\n",
    "    def state_to_tuple(self, current_state):\n",
    "        # convert current_state to tuple\n",
    "        current_state_tup = []\n",
    "        current_state_tup.append(('pos', current_state['pos']))\n",
    "        current_state_tup.append(('holding', current_state['holding']))\n",
    "        current_state_tup.append(('objects_in_g1', current_state['objects_in_g1']))\n",
    "        current_state_tup.append(('objects_in_g2', current_state['objects_in_g2']))\n",
    "\n",
    "\n",
    "\n",
    "        return tuple(current_state_tup)\n",
    "\n",
    "    def tuple_to_state(self, current_state_tup):\n",
    "        # convert current_state to tuple\n",
    "        current_state_tup = list(current_state_tup)\n",
    "        current_state = {}\n",
    "        current_state['pos'] = current_state_tup[0][1]\n",
    "        # current_state['square_positions'] = current_state_tup[1][1]\n",
    "        # current_state['triangle_positions'] = current_state_tup[2][1]\n",
    "        current_state['holding'] = current_state_tup[1][1]\n",
    "        current_state['objects_in_g1'] = current_state_tup[2][1]\n",
    "        current_state['objects_in_g2'] = current_state_tup[3][1]\n",
    "        # current_state['g1'] = current_state_tup[6][1]\n",
    "        # current_state['g2'] = current_state_tup[7][1]\n",
    "        # current_state['target_object'] = current_state_tup[8][1]\n",
    "        # current_state['target_goal'] = current_state_tup[9][1]\n",
    "\n",
    "\n",
    "        return current_state\n",
    "\n",
    "    def enumerate_states(self):\n",
    "        self.reset()\n",
    "\n",
    "        actions = self.possible_single_actions\n",
    "        # print(\"actions\", actions)\n",
    "        # pdb.set_trace()\n",
    "        # create directional graph to represent all states\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        visited_states = set()\n",
    "\n",
    "        stack = [copy.deepcopy(self.current_state)]\n",
    "\n",
    "        while stack:\n",
    "            # print(\"len visited_states\", len(visited_states))\n",
    "            # print(\"len stack\", len(stack))\n",
    "            # print(\"visited_states\", visited_states)\n",
    "            state = stack.pop()\n",
    "\n",
    "            # convert old state to tuple\n",
    "            state_tup = self.state_to_tuple(state)\n",
    "            # print(\"new_state_tup\", state_tup)\n",
    "\n",
    "            # if state has not been visited, add it to the set of visited states\n",
    "            if state_tup not in visited_states:\n",
    "                visited_states.add(state_tup)\n",
    "\n",
    "            # get the neighbors of this state by looping through possible actions\n",
    "            # actions = self.get_possible_actions_in_state(state)\n",
    "            # print(\"POSSIBLE actions\", actions)\n",
    "            for idx, action in enumerate(actions):\n",
    "                # print(\"action\", action)\n",
    "                if self.is_done_given_state(state):\n",
    "                    team_reward = 0\n",
    "                    next_state = state\n",
    "                    done = True\n",
    "\n",
    "                else:\n",
    "                    next_state, team_reward, done = self.step_given_state(state, action)\n",
    "                # print(\"state\", state)\n",
    "                # print(\"action\", action)\n",
    "                # print(\"team_reward\", team_reward)\n",
    "                # print(\"done\", done)\n",
    "                # print(\"next_state\", next_state)\n",
    "\n",
    "\n",
    "                # if done:\n",
    "                #     print(\"DONE\")\n",
    "                #     print(\"team_reward\", team_reward)\n",
    "                #\n",
    "                #     print(\"state\", state)\n",
    "                #     print(\"next_state\", next_state)\n",
    "                #     print(\"action\", action)\n",
    "                #     print()\n",
    "                #     team_reward += 10\n",
    "\n",
    "                new_state_tup = self.state_to_tuple(next_state)\n",
    "                # print(\"new_state_tup\", new_state_tup)\n",
    "                # print(\"new_state_tup in visited_states = \", new_state_tup in visited_states)\n",
    "                # print()\n",
    "                # pdb.set_trace()\n",
    "\n",
    "                if new_state_tup not in visited_states:\n",
    "                    stack.append(copy.deepcopy(next_state))\n",
    "\n",
    "                # add edge to graph from current state to new state with weight equal to reward\n",
    "                # if state_tup == new_state_tup:\n",
    "                G.add_edge(state_tup, new_state_tup, weight=team_reward, action=action)\n",
    "                # if state == {'grid': {(1, 1): (3, 3)}, 'exit': False, 'orientation': 0}:\n",
    "                #     el = G.out_edges(state_tup, data=True)\n",
    "                #     print(\"len el\", len(el))\n",
    "                #     if action == (1,0):\n",
    "                #         pdb.set_trace()\n",
    "                #     G.add_edge(state_tup, new_state_tup, weight=team_reward, action=str(action))\n",
    "                #\n",
    "                #     el = G.out_edges(state_tup, data=True)\n",
    "                #     print(\"new len el\", len(el))\n",
    "                #     print(\"el\", el)\n",
    "                #     print(\"action\", action)\n",
    "                #     print()\n",
    "\n",
    "                # if state_tup == new_state_tup:\n",
    "                #     pdb.set_trace()\n",
    "                # if state_tup != new_state_tup:\n",
    "                #     G.add_edge(state_tup, new_state_tup, weight=team_reward, action=action)\n",
    "                # if state_tup == new_state_tup:\n",
    "                #     if self.is_done_given_state(state) is False:\n",
    "                #         G.add_edge(state_tup, new_state_tup, weight=-200, action=action)\n",
    "                #     else:\n",
    "                #         G.add_edge(state_tup, new_state_tup, weight=0, action=action)\n",
    "                        # pdb.set_trace()\n",
    "        # pdb.set_trace()\n",
    "        states = list(G.nodes)\n",
    "        # print(\"NUMBER OF STATES\", len(state\n",
    "        idx_to_state = {i: state for i, state in enumerate(states)}\n",
    "        state_to_idx = {state: i for i, state in idx_to_state.items()}\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        action_to_idx = {action: i for i, action in enumerate(actions)}\n",
    "        idx_to_action = {i: action for i, action in enumerate(actions)}\n",
    "\n",
    "        # construct transition matrix and reward matrix of shape [# states, # states, # actions] based on graph\n",
    "        transition_mat = np.zeros([len(states), len(states), len(actions)])\n",
    "        reward_mat = np.zeros([len(states), len(actions)])\n",
    "\n",
    "        for i in range(len(states)):\n",
    "            # get all outgoing edges from current state\n",
    "            # edges = G.out_edges(states[i], data=True)\n",
    "            # if self.tuple_to_state(idx_to_state[i]) == {'grid': {(1, 1): (3, 3)}, 'exit': False, 'orientation': 0}:\n",
    "            #     edges = G.out_edges(states[i], data=True)\n",
    "            #     print(\"edges= \", edges)\n",
    "            #     pdb.set_trace()\n",
    "            state = self.tuple_to_state(idx_to_state[i])\n",
    "            for action_idx_i in range(len(actions)):\n",
    "                action = idx_to_action[action_idx_i]\n",
    "                if self.is_done_given_state(state):\n",
    "                    team_reward = 0\n",
    "                    next_state = state\n",
    "                    done = True\n",
    "\n",
    "                else:\n",
    "                    next_state, team_reward, done = self.step_given_state(state, action)\n",
    "            # for edge in edges:\n",
    "                # get index of action in action_idx\n",
    "                # pdb.set_trace()\n",
    "                # action_idx_i = action_to_idx[edge[2]['action']]\n",
    "                # get index of next state in node list\n",
    "                # next_state_i = states.index(edge[1])\n",
    "                next_state_i = state_to_idx[self.state_to_tuple(next_state)]\n",
    "                # add edge to transition matrix\n",
    "                # if i == next_state_i:\n",
    "                #     reward_mat[i, action_idx_i] = -200\n",
    "                # else:\n",
    "                #     reward_mat[i, action_idx_i] = edge[2]['weight']\n",
    "                #     transition_mat[i, next_state_i, action_idx_i] = 0.0\n",
    "                #\n",
    "                # else:\n",
    "                transition_mat[i, next_state_i, action_idx_i] = 1.0\n",
    "                # reward_mat[i, action_idx_i] = edge[2]['weight']\n",
    "                reward_mat[i, action_idx_i] = team_reward\n",
    "                # pdb.set_trace()\n",
    "                # if idx_to_action[action_idx_i] == (0, 1) and self.tuple_to_state(idx_to_state[i]) == {'grid': {(1, 1): (3, 3)}, 'exit': False, 'orientation': 0}:\n",
    "                #     # reward_mat[i, action_idx_i] = 0.0\n",
    "                #     pdb.set_trace()\n",
    "                # if self.tuple_to_state(idx_to_state[i]) == {'grid': {(1, 1): (3, 3)}, 'exit': False, 'orientation': 0}:\n",
    "                #     edges = G.out_edges(states[i], data=True)\n",
    "                #     print(\"edges= \", edges)\n",
    "                #     print(\"action\", idx_to_action[action_idx_i])\n",
    "                    # pdb.set_trace()\n",
    "\n",
    "        # check that for each state and action pair, the sum of the transition probabilities is 1 (or 0 for terminal states)\n",
    "        # for i in range(len(states)):\n",
    "        #     for j in range(len(actions)):\n",
    "        #         print(\"np.sum(transition_mat[i, :, j])\", np.sum(transition_mat[i, :, j]))\n",
    "        #         print(\"np.sum(transition_mat[i, :, j]\", np.sum(transition_mat[i, :, j]))\n",
    "        # assert np.isclose(np.sum(transition_mat[i, :, j]), 1.0) or np.isclose(np.sum(transition_mat[i, :, j]),\n",
    "        #                                                                       0.0)\n",
    "        self.transitions, self.rewards, self.state_to_idx, \\\n",
    "        self.idx_to_action, self.idx_to_state, self.action_to_idx = transition_mat, reward_mat, state_to_idx, \\\n",
    "                                                                    idx_to_action, idx_to_state, action_to_idx\n",
    "\n",
    "        # print(\"number of states\", len(states))\n",
    "        # print(\"number of actions\", len(actions))\n",
    "        # print(\"transition matrix shape\", transition_mat.shape)\n",
    "        return transition_mat, reward_mat, state_to_idx, idx_to_action, idx_to_state, action_to_idx\n",
    "\n",
    "    def vectorized_vi(self):\n",
    "        # def spatial_environment(transitions, rewards, epsilson=0.0001, gamma=0.99, maxiter=10000):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "            transitions : array_like\n",
    "                Transition probability matrix. Of size (# states, # states, # actions).\n",
    "            rewards : array_like\n",
    "                Reward matrix. Of size (# states, # actions).\n",
    "            epsilson : float, optional\n",
    "                The convergence threshold. The default is 0.0001.\n",
    "            gamma : float, optional\n",
    "                The discount factor. The default is 0.99.\n",
    "            maxiter : int, optional\n",
    "                The maximum number of iterations. The default is 10000.\n",
    "        Returns\n",
    "        -------\n",
    "            value_function : array_like\n",
    "                The value function. Of size (# states, 1).\n",
    "            pi : array_like\n",
    "                The optimal policy. Of size (# states, 1).\n",
    "        \"\"\"\n",
    "        n_states = self.transitions.shape[0]\n",
    "        n_actions = self.transitions.shape[2]\n",
    "\n",
    "        # initialize value function\n",
    "        pi = np.zeros((n_states, 1))\n",
    "        vf = np.zeros((n_states, 1))\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "        policy = {}\n",
    "\n",
    "        for i in range(self.maxiter):\n",
    "            # initalize delta\n",
    "            delta = 0\n",
    "            # perform Bellman update\n",
    "            for s in range(n_states):\n",
    "                # store old value function\n",
    "                old_v = vf[s].copy()\n",
    "                # compute new value function\n",
    "                Q[s] = np.sum((self.rewards[s] + self.gamma * vf) * self.transitions[s, :, :], 0)\n",
    "                vf[s] = np.max(np.sum((self.rewards[s] + self.gamma * vf) * self.transitions[s, :, :], 0))\n",
    "                # compute delta\n",
    "                delta = np.max((delta, np.abs(old_v - vf[s])[0]))\n",
    "            # check for convergence\n",
    "            if delta < self.epsilson:\n",
    "                break\n",
    "        # compute optimal policy\n",
    "        for s in range(n_states):\n",
    "            pi[s] = np.argmax(np.sum(vf * self.transitions[s, :, :], 0))\n",
    "            policy[s] = Q[s, :]\n",
    "\n",
    "        self.vf = vf\n",
    "        self.pi = pi\n",
    "        self.policy = policy\n",
    "        return vf, pi\n",
    "\n",
    "    def rollout_full_game_joint_optimal(self):\n",
    "        self.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        iters = 0\n",
    "        game_results = []\n",
    "        sum_feature_vector = np.zeros(4)\n",
    "\n",
    "        # self.render(self.current_state, iters)\n",
    "        while not done:\n",
    "            iters += 1\n",
    "\n",
    "            current_state_tup = self.state_to_tuple(self.current_state)\n",
    "\n",
    "            state_idx = self.state_to_idx[current_state_tup]\n",
    "\n",
    "            action_distribution = self.policy[state_idx]\n",
    "            action = np.argmax(action_distribution)\n",
    "            action = self.idx_to_action[action]\n",
    "\n",
    "            # print(\"state\", self.current_state)\n",
    "            # print(\"action\", action)\n",
    "\n",
    "            game_results.append((self.current_state, action))\n",
    "\n",
    "            next_state, team_rew, done = self.step_given_state(self.current_state, action)\n",
    "\n",
    "            # print(\"next_state\", next_state)\n",
    "            self.current_state = next_state\n",
    "            # print(\"new state\", self.current_state)\n",
    "            # print(\"team_rew\", team_rew)\n",
    "            # print(\"done\", done)\n",
    "            # print()\n",
    "            # self.render(self.current_state, iters)\n",
    "\n",
    "            total_reward += team_rew\n",
    "\n",
    "            if iters > 40:\n",
    "                break\n",
    "\n",
    "        # self.save_rollouts_to_video()\n",
    "\n",
    "        return total_reward, game_results\n",
    "\n",
    "\n",
    "    def compute_optimal_performance(self):\n",
    "        # print(\"start enumerating states\")\n",
    "        self.enumerate_states()\n",
    "        # print(\"done enumerating states\")\n",
    "        # print(\"start vi\")\n",
    "        self.vectorized_vi()\n",
    "        # print(\"done vi\")\n",
    "\n",
    "        optimal_rew, game_results = self.rollout_full_game_joint_optimal()\n",
    "        return optimal_rew, game_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
