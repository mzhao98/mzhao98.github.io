{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from array import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN = -np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphaVector(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper for the alpha vector used for representing the value function for a POMDP as a piecewise-linear,\n",
    "    convex function\n",
    "    \"\"\"\n",
    "    def __init__(self, a, v):\n",
    "        self.action = a\n",
    "        self.v = v\n",
    "\n",
    "    def copy(self):\n",
    "        return AlphaVector(self.action, self.v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "\n",
    "human_prefers_pans, human_prefers_plates = 0,1\n",
    "robot_takes_pan, robot_takes_plate = 0,1\n",
    "human_takes_pan, human_takes_plate = 0,1\n",
    "\n",
    "states = [human_prefers_pans, human_prefers_plates]\n",
    "actions = [robot_takes_pan, robot_takes_plate]\n",
    "observations = [human_takes_pan, human_takes_plate]\n",
    "\n",
    "states_text = {human_prefers_pans: 'human_prefers_pans', human_prefers_plates: 'human_prefers_plates'}\n",
    "actions_text = {robot_takes_pan: 'robot_takes_pan', robot_takes_plate: 'robot_takes_plate'}\n",
    "observations_text = {human_takes_pan: 'human_takes_pan', human_takes_plate: 'human_takes_plate'}\n",
    "\n",
    "num_states = 2\n",
    "num_actions = 2\n",
    "num_observations = 2\n",
    "\n",
    "T = np.array([[[1.0,0.0] , [0.0,1.0] ],\n",
    "    [[1.0,0.0] , [0.0,1.0] ]])\n",
    "# T = np.swapaxes(T, 0,1) # state, action, next state\n",
    "\n",
    "O = np.array([[[0.85, 0.15] , [0.15, 0.85] ],\n",
    "    [[0.85, 0.15] , [0.15, 0.85] ]])\n",
    "# O = np.swapaxes(O, 0,1) # state, action, obs\n",
    "\n",
    "\n",
    "R = np.array([[-100.0, 10.0 ],\n",
    "    [10.0, -100.0 ]])\n",
    "# R = np.swapaxes(R, 0,1) # state, action\n",
    "\n",
    "C = [-1, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pomdp(max_timesteps, alpha_vecs, gamma_reward):\n",
    "\n",
    "    for step in range(max_timesteps):\n",
    "\n",
    "        # First compute a set of updated vectors for every action/observation pair\n",
    "        gamma_intermediate = {}\n",
    "        for a in actions:\n",
    "            gamma_intermediate[a] = {}\n",
    "            for o in observations:\n",
    "    #             gamma_action_obs = compute_gamma_action_obs(a, o)\n",
    "\n",
    "                gamma_action_obs = []\n",
    "                for alpha in alpha_vecs:\n",
    "                    v = np.zeros(num_states)  # initialize the update vector [0, ... 0]\n",
    "                    for i, si in enumerate(states):\n",
    "                        for j, sj in enumerate(states):\n",
    "                            v[i] += T[a, si, sj] * O[a, sj, o] * alpha.v[j]\n",
    "                        v[i] *= discount\n",
    "                    gamma_action_obs.append(v)\n",
    "\n",
    "                gamma_intermediate[a][o] = gamma_action_obs\n",
    "\n",
    "        # Now compute the cross sum\n",
    "        gamma_action_belief = {}\n",
    "        for a in actions:\n",
    "\n",
    "            gamma_action_belief[a] = {}\n",
    "            for bidx, b in enumerate(belief_points):\n",
    "\n",
    "                gamma_action_belief[a][bidx] = gamma_reward[a].copy()\n",
    "    #             print(\"gamma_reward[a].copy()\", gamma_reward[a].copy())\n",
    "\n",
    "                for o in observations:\n",
    "                    # only consider the best point\n",
    "                    best_alpha_idx = np.argmax(np.dot(gamma_intermediate[a][o], b))\n",
    "    #                 print(\"gamma_intermediate[a][o][best_alpha_idx]\", gamma_intermediate[a][o][best_alpha_idx])\n",
    "                    gamma_action_belief[a][bidx] += gamma_intermediate[a][o][best_alpha_idx]\n",
    "\n",
    "\n",
    "        # Finally compute the new(best) alpha vector set\n",
    "        alpha_vecs, max_val = [], MIN\n",
    "\n",
    "        for bidx, b in enumerate(belief_points):\n",
    "            best_av, best_aa = None, None\n",
    "\n",
    "            for a in actions:\n",
    "                val = np.dot(gamma_action_belief[a][bidx], b)\n",
    "                if best_av is None or val > max_val:\n",
    "                    max_val = val\n",
    "                    best_av = gamma_action_belief[a][bidx].copy()\n",
    "                    best_aa = a\n",
    "\n",
    "            alpha_vecs.append(AlphaVector(a=best_aa, v=best_av))\n",
    "\n",
    "\n",
    "    return alpha_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup alpha vectors\n",
    "alpha_vecs = [AlphaVector(a=-1, v=np.zeros(num_states))] \n",
    "stepsize = 0.1\n",
    "belief_points = [[np.random.uniform() for s in states] for p in np.arange(0., 1. + stepsize, stepsize)]\n",
    "\n",
    "# gamma_reward_og = {\n",
    "#             a: np.frombuffer(array('d', [R[s,a] for s in states]))\n",
    "#             for a in actions\n",
    "#         }\n",
    "\n",
    "gamma_reward = {\n",
    "            a: np.array( [R[a,s] for s in states])\n",
    "            for a in actions\n",
    "        }\n",
    "\n",
    "max_timesteps = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# belief = ctx.random_beliefs() if params.random_prior else ctx.generate_beliefs()\n",
    "rand_nums = np.random.randint(0, 100, size=num_states)\n",
    "base = sum(rand_nums)*1.0\n",
    "belief = [x/base for x in rand_nums]\n",
    "belief = [0.5, 0.5]\n",
    "state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief [0.5, 0.5]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_plate\n",
      "reward  -100.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [0.15, 0.85]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [0.0302013422818792, 0.9697986577181208]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [0.005465587044534414, 0.9945344129554656]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [0.0009688763426712281, 0.9990311236573287]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [0.00017111471023167384, 0.9998288852897683]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [3.0200969430404745e-05, 0.9999697990305696]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [5.32971539807174e-06, 0.9999946702846019]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [9.405421396307151e-07, 0.9999990594578604]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [1.6597815320143577e-07, 0.9999998340218468]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [2.929026633328986e-08, 0.9999999707097337]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [5.168870654084467e-09, 0.9999999948311293]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [9.121536487212155e-10, 0.9999999990878464]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_pan\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [5.168870654084467e-09, 0.9999999948311293]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [9.121536487212155e-10, 0.9999999990878464]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [1.6096829107171978e-10, 0.9999999998390318]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [2.840616901642201e-11, 0.9999999999715938]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [5.012853355956445e-12, 0.9999999999949871]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [8.846211804665539e-13, 0.9999999999991154]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n",
      "belief [1.561096200824468e-13, 0.9999999999998439]\n",
      "state  human_prefers_plates\n",
      "action  robot_takes_pan\n",
      "reward  10.0\n",
      "next_obs  human_takes_plate\n",
      "next_state  human_prefers_plates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_rewards = 0\n",
    "budget = 20\n",
    "max_play = 20\n",
    "for i in range(max_play):\n",
    "    # plan, take action and receive environment feedbacks\n",
    "    alpha_vecs = solve_pomdp(max_timesteps, alpha_vecs, gamma_reward) \n",
    "    print(\"belief\", belief)\n",
    "#     action = pomdp.get_action(belief)\n",
    "    \n",
    "#     print(\"ALPHA VECS\")\n",
    "    for av in alpha_vecs:\n",
    "        v = np.dot(av.v, belief)\n",
    "#         print(\"action: \", av.action)\n",
    "#         print(\"v: \", v)\n",
    "    \n",
    "    \n",
    "    max_v = -np.inf\n",
    "    best = None\n",
    "    for av in alpha_vecs:\n",
    "        v = np.dot(av.v, belief)\n",
    "        if v > max_v:\n",
    "            max_v = v\n",
    "            best = av\n",
    "    action = best.action\n",
    "    \n",
    "    print(\"state \", states_text[state])\n",
    "    print(\"action \", actions_text[action])\n",
    "    \n",
    "    \n",
    "    ai = action\n",
    "    si = state\n",
    "    # get new state\n",
    "    s_probs = [T[ai, si, sj] for sj in states]\n",
    "    next_state = states[np.random.choice(np.arange(num_states), p=s_probs)]\n",
    "\n",
    "    # get new observation\n",
    "    o_probs = [O[ai, next_state, oj] for oj in observations]\n",
    "    next_obs = observations[np.random.choice(np.arange(num_observations), p=o_probs)]\n",
    "\n",
    "    next_reward = R[ai, si]  \n",
    "    next_cost = C[ai]\n",
    "    \n",
    "    print(\"reward \", next_reward)\n",
    "    print(\"next_obs \", observations_text[next_obs])\n",
    "    print(\"next_state \", states_text[next_state])\n",
    "    print()\n",
    "        \n",
    "    new_state, obs, reward, cost = next_state, next_obs, next_reward, next_cost\n",
    "#     state, observation, reward, cost = self.simulate_action(curr_state, action)\n",
    "#     curr_state = state\n",
    "        \n",
    "#     new_state, obs, reward, cost = pomdp.take_action(action)\n",
    "\n",
    "    # update states\n",
    "#     belief = pomdp.update_belief(belief, action, obs)\n",
    "\n",
    "    b_new = []\n",
    "    for sj in states:\n",
    "        p_o_prime = O[action, sj, obs]\n",
    "        summation = 0.0\n",
    "        for i, si in enumerate(states):\n",
    "            p_s_prime = T[action, si, sj]\n",
    "            summation += p_s_prime * float(belief[i])\n",
    "        b_new.append(p_o_prime * summation)\n",
    "\n",
    "    # normalize\n",
    "    total = sum(b_new)\n",
    "    belief = [x / total for x in b_new]\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_rewards += reward\n",
    "    budget -= cost\n",
    "\n",
    "    if budget <= 0:\n",
    "        print('Budget spent.')\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
