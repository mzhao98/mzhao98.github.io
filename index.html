<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Michelle Zhao</title>

    <meta name="author" content="Michelle Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--      <link rel="icon" href="images/robot_icon.png,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">-->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Michelle Zhao
                </p>
                <p>Hi, I am a fourth-year PhD student in the Robotics Institute at Carnegie Mellon University, working
                  on human-robot collaboration. I am advised by
                  <a href="https://www.ri.cmu.edu/ri-faculty/reid-simmons/">Reid Simmons</a> and <a href="https://www.ri.cmu.edu/ri-faculty/henny-admoni/">Henny Admoni</a>,
                  and am part of the <a href="https://harp.ri.cmu.edu"> Human and Robot Partners (HARP) Lab</a> and
                  <a href="https://www.ri.cmu.edu/robotics-groups/reliable-autonomous-systems-lab/"> Reliable Autonomous Systems Lab (RASL)</a>. I am supported
                  by the <a href="https://ndseg.org/">National Defense Science and Engineering Graduate (NDSEG) Fellowship</a>.
                </p>
                <p>
                  My research interests lie in the areas of interactive machine learning, reinforcement learning,
                  and human robot interaction. I graduated from Caltech ('20) with a degree in Computer Science and a
                  minor in Information and Data Science, where I worked with <a href="https://yuxinchen.org/">Yuxin Chen</a> and
                  <a href="http://www.yisongyue.com/">Yisong Yue</a> on machine teaching and
                  <a href="https://www.eas.caltech.edu/people/sjchung">Soon-Jo Chung</a>
                  on distributed control.
                </p>
                <p style="text-align:center">
                  <a href="mailto:mzhao2@andrew.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/mz_resume_nov2023.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/MZ_bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Gu4eXZwAAAAJ&hl=en&oi=sra">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/michelledzhao">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mzhao98/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/circular_prof_pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/circular_prof_pic.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in developing algorithms to enable fluent human-robot coordination and collaboration.
                  I currently work on projects focused on uncertainty quantification, human-robot collaboration, adaptation, and modeling human behavior.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cdagger_real.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2406.07767">
                  <span class="papertitle">Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni, Aaditya Ramdas*, Andrea Bajcsy*
                <br>
                <em>Under review, ICLR</em>, 2025
                <br>
                <a href="https://cmu-intentlab.github.io/conformalized-interactive-il/">project site</a> /
<!--                <a href="https://arxiv.org/pdf/2406.07767">paper</a> /-->
                <a href="data/zhao_conformalized_iclr_2025.bib">bibtex</a>
<!--                <a href="https://www.youtube.com/watch?v=rkydo1yUGZY">video</a>-->
                <p>
                    In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot)
                    to contend with distribution shifts encountered during deployment by actively seeking additional feedback
                    from an expert (i.e. human) online. From the conformal prediction side, we introduce a novel uncertainty
                    quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic
                    model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves
                    desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach
                    wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time
                    uncertainty to actively query for more expert feedback.
                </p>
              </td>
          </tr>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/con_teleop.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2406.07767">
                  <span class="papertitle">Conformalized Teleoperation: Confidently Mapping Human Inputs to High-Dimensional Robot Action</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni, Andrea Bajcsy
                <br>
                <em>RSS</em>, 2024
                <br>
                <a href="https://cmu-intentlab.github.io/conformalized-teleoperation/">project site</a> /
                <a href="https://arxiv.org/pdf/2406.07767">paper</a> /
                <a href="data/zhao_conformalized_rss_2024.bib">bibtex</a> /
                <a href="https://www.youtube.com/watch?v=rkydo1yUGZY">video</a>
                <p>
                    Assistive robotic arms often have more degrees-of-freedom than a human teleoperator can control with
                    a low-dimensional input, like a joystick. To overcome this challenge, existing approaches use
                    data-driven methods to learn a mapping from low-dimensional human inputs to high-dimensional
                    robot actions. However, determining if such a black-box mapping can confidently infer a user's
                    intended high-dimensional action from low-dimensional inputs remains an open problem. Our key idea
                    is to adapt the assistive map at training time to additionally estimate high-dimensional action
                    quantiles, and then calibrate these quantiles via rigorous uncertainty quantification methods.
                </p>
              </td>
          </tr>




          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/strats2.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=7CtUcT_OHmC">
                  <span class="papertitle">Multi-Agent Strategy Explanations for Human-Robot Collaboration</span>
                </a>
                <br>
                Ravi Pandya*, <strong>Michelle Zhao*</strong>, Changliu Liu, Reid Simmons, Henny Admoni
                <br>
                <em>ICRA</em>, 2024
                <br>
                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Gu4eXZwAAAAJ&citation_for_view=Gu4eXZwAAAAJ:W7OEmFMy1HYC">paper</a> /
                <a href="data/zhao_topics_adaptation_2022.bib">bibtex</a>
<!--                <a href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing">poster</a> /-->
<!--                <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>-->
                <p>
                    In this work, we investigate how to generate multi-agent strategy explanations for
                    human-robot collaboration. We formulate the problem using a generic multi-agent planner, show how
                    to generate visual explanations through strategy-conditioned landmark states and generate textual
                    explanations by giving the landmarks to an LLM.
                </p>
              </td>
          </tr>




          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/baisl_robots.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=7CtUcT_OHmC">
                  <span class="papertitle">Learning Human Contribution Preferences in Collaborative Human-Robot Tasks</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni
                <br>
                <em>CORL</em>, 2023
                <br>
                <a href="https://openreview.net/pdf?id=7CtUcT_OHmC">paper</a> /
                <a href="data/zhao_topics_adaptation_2022.bib">bibtex</a>
<!--                <a href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing">poster</a> /-->
<!--                <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>-->
                <p>
                  We propose a method for representing human and robot contribution constraints in collaborative human-robot tasks. Additionally,
                  we present an approach for learning a human partner's contribution constraint online during a
                  collaborative interaction. We evaluate our approach using a variety of simulated human partners in
                  a collaborative decluttering task.
                </p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/zhao_topics_cover_game.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://onlinelibrary.wiley.com/doi/10.1111/tops.12633">
                  <span class="papertitle">The Role of Adaptation in Collective Human‚ÄìAI Teaming</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni
                <br>
                <em>Topics in Cognitive Science</em>, 2022
                <br>
                <a href="data/zhao_adaptation_topics_2022.pdf">paper</a> /
                <a href="data/zhao_topics_adaptation_2022.bib">bibtex</a>
<!--                <a href="https://drive.google.com/file/d/12x8mhqpFsA6p0u6ZQW-ieRKF8hlQBKKe/view?usp=sharing">poster</a> /-->
<!--                <a href="http://www.youtube.com/watch?v=NnePYprvFvA">video</a>-->
                <p>
                  This paper presents a framework for defining artificial intelligence (AI) that adapts to individuals within a group,
                  and discusses the technical challenges for collaborative AI systems that must work with different human partners.
                  Collaborative AI is not one-size-fits-all, and thus AI systems must tune their output based on each human partner's
                  needs and abilities.
                </p>
              </td>
            </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/zhao_coordination_cover.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/iel7/9981026/9981028/09982277.pdf">
                  <span class="papertitle">Coordination With Humans Via Strategy Matching</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni
                <br>
                <em>IROS</em>, 2022
                <br>
                <a href="data/zhao_coordination_iros2022.pdf">paper</a> /
                <a href="data/zhao_topics_adaptation_2022.bib">bibtex</a> /
                <a href="https://www.youtube.com/watch?v=j40Mpg8wiuU">video</a>
                <p>
                  This work autonomously recognizes available task-completion strategies by observing human-human teams performing a collaborative task. By
                  transforming team actions into low dimensional representations using hidden Markov models, we can
                  identify strategies without prior knowledge. Robot policies are learned on each of the identified
                  strategies to construct a Mixture-of-Experts model that adapts to the task strategies of unseen human
                  partners.
                </p>
              </td>
            </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/minimap.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0747563222003442">
                  <span class="papertitle">Teaching Agents to Understand Teamwork: Evaluating and Predicting Collective
                    Intelligence as a Latent Variable via Hidden Markov Models</span>
                </a>
                <br>
                <strong>Michelle Zhao*</strong>, Fade Eadeh*, Thuy-Ngoc Nguyen, Pranay Gupta, Henny Admoni, Cleotide Gonzalez, Anita Williams Woolley
                <br>
                <em>Computers in Human Behavior</em>, 2022
                <br>
                <a href="data/zhao_teaching_chb_2022.pdf">paper</a> /
                <a href="data/zhao_topics_adaptation_2022.bib">bibtex</a>
                <p>We show by learning the set of hidden states representing a team‚Äôs observed collaborative process
                  behaviors over time, we both learn information about the team‚Äôs collective intelligence (CI), predict how CI will evolve in
                  the future, and suggest when an agent might intervene to improve team performance.
                </p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Workshop Papers</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/teaching_diagram.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
<!--                <a href="https://ai4athome.github.io/res/papers/chen_learning_human_preferences.pdf">-->
                  <span class="papertitle">Machine Teaching of Collaborative Policies for Human Inverse Reinforcement Learning</span>
                </a>
                <br>
                Nyomi Morris, <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni
                <br>
                <em>RL-CONFORM Workshop: RL Meets HRI, Control, and Formal Methods (IROS)</em>, 2023
                <br>
                <a href="https://rlconform-workshop.github.io/pdfs/4_2023_RL_Conform_Camera_Ready.pdf">workshop paper</a>
                  <!--                  make text red and bold-->
                <p style="color: red">
<!--                  make red and bold-->
                    <strong>Best Poster Presentation Award</strong>

                </p>
                <p>
                  We consider the problem of teaching a human partner a joint reward function, which captures how both
                  human and robot should contribute to the task. This reward, which is known only to the robot, is
                  joint over human and robot actions, and encompasses constraints over how the human and robot should
                  contribute to a task. By adapting existing machine teaching frameworks for our collaborative domain,
                  we seek to provide a minimal number of demonstrations such that a human can learn the rewards.
                </p>

              </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ai2thor.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ai4athome.github.io/res/papers/chen_learning_human_preferences.pdf">
                  <span class="papertitle">Learning Human Preferences for Personalized Assistance in Household Tasks</span>
                </a>
                <br>
                Daphne Chen, <strong>Michelle Zhao</strong>, Reid Simmons
                <br>
                <em>AAAI Workshop on User-Centric Artificial Intelligence for Assistance in At-Home Tasks</em>, 2023
                <br>
                <a href="https://ai4athome.github.io/res/papers/chen_learning_human_preferences.pdf">workshop paper</a>
                <p>
                  We propose a method for generating a customizable quantity of synthetic data that reflects the variability
                  in task execution styles seen in the real-world task, and enables us to train a baseline sequential
                  model that predicts the next action a participant will take within a cooking activity.
                </p>
              </td>
          </tr>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/loops.png" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="http://packages.personalrobotics.ri.cmu.edu/assets/pubs/zhao_adapting_hri_leap_workshop_2021.pdf">
                  <span class="papertitle">Adapting Language Complexity for Ai-Based Assistance</span>
                </a>
                <br>
                <strong>Michelle Zhao</strong>, Reid Simmons, Henny Admoni
                <br>
                <em>ACM/IEEE International Conference on Human Robot Interaction, Lifelong Learning and Personalization in Long-Term Human-Robot Interaction (LEAP-HRI)</em>, 2021
                <br>
                <a href="http://packages.personalrobotics.ri.cmu.edu/assets/pubs/zhao_adapting_hri_leap_workshop_2021.pdf">workshop paper</a>
                <p>
                  We present a closed-loop interaction framework that adapts the level of information complexity based
                  on the human partner‚Äôs observable cognitive understanding. This work investigates how
                  knowledge and preparation impact the suitability of di erent complexity levels, motivating dynamic interaction.
                </p>
              </td>
            </tr>


          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Other</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


            <tr>
<!--              <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--                <img src="images/teaching.png" alt="teaching">-->
<!--              </td>-->
              <td width="75%" valign="center">
                Teaching Assistant, Graduate Human Robot Interaction, CMU, Fall 2022
                <br>
                Teaching Assistant, Undergraduate Human Robot Interaction, CMU, Spring 2022
                <br>
                Teaching Assistant, Networks: Structure and Economics, Caltech, Winter 2020
                <br>
                Teaching Assistant, Machine Learning and Data Mining, Caltech, Winter 2019
                <br>
                Teaching Assistant, Machine Learning Systems, Caltech, Fall 2018
                <br>
                Teaching Assistant, Machine Learning Systems, Caltech, Fall 2018

              </td>
            </tr>
            
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


<!--            <tr>-->
<!--&lt;!&ndash;              <td style="padding:20px;width:25%;vertical-align:middle">&ndash;&gt;-->
<!--&lt;!&ndash;                <img src="images/teaching.png" alt="teaching">&ndash;&gt;-->
<!--&lt;!&ndash;              </td>&ndash;&gt;-->
<!--              <td width="75%" valign="center">-->
<!--                <a href="tutorials/hri_intro.html">Tutorial</a>-->

<!--              </td>-->
<!--            </tr>-->

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
