{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from array import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN = -np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphaVector(object):\n",
    "    \"\"\"\n",
    "    Simple wrapper for the alpha vector used for representing the value function for a POMDP as a piecewise-linear,\n",
    "    convex function\n",
    "    \"\"\"\n",
    "    def __init__(self, a, v):\n",
    "        self.action = a\n",
    "        self.v = v\n",
    "\n",
    "    def copy(self):\n",
    "        return AlphaVector(self.action, self.v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "\n",
    "tiger_left, tiger_right = 0,1\n",
    "open_left, open_right, listen = 0,1,2\n",
    "growl_left, growl_right = 0,1\n",
    "\n",
    "states = [tiger_left, tiger_right]\n",
    "actions = [open_left, open_right, listen]\n",
    "observations = [growl_left, growl_right]\n",
    "\n",
    "states_text = {tiger_left: 'tiger_left', tiger_right: 'tiger_right'}\n",
    "actions_text = {open_left: 'open_left', open_right: 'open_right', listen:'listen'}\n",
    "observations_text = {growl_left: 'growl_left', growl_right: 'growl_right'}\n",
    "\n",
    "num_states = 2\n",
    "num_actions = 2\n",
    "num_observations = 2\n",
    "\n",
    "T = np.array([[[0.5, 0.5] , [0.5, 0.5] ],\n",
    "    [[0.5, 0.5] , [0.5, 0.5] ],\n",
    "    [[1.0,0.0] , [0.0,1.0] ]])\n",
    "# T = np.swapaxes(T, 0,1) # state, action, next state\n",
    "\n",
    "O = np.array([[[0.5, 0.5] , [0.5, 0.5] ],\n",
    "    [[0.5, 0.5] , [0.5, 0.5] ],\n",
    "    [[0.85, 0.15] , [0.15, 0.85] ]])\n",
    "# O = np.swapaxes(O, 0,1) # state, action, obs\n",
    "\n",
    "\n",
    "R = np.array([[-100.0, 10.0 ],\n",
    "    [10.0, -100.0 ],\n",
    "    [-1.0, -1.0 ]])\n",
    "# R = np.swapaxes(R, 0,1) # state, action\n",
    "\n",
    "C = [-1, -1, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_pomdp(max_timesteps, alpha_vecs, gamma_reward):\n",
    "\n",
    "    for step in range(max_timesteps):\n",
    "\n",
    "        # First compute a set of updated vectors for every action/observation pair\n",
    "        gamma_intermediate = {}\n",
    "        for a in actions:\n",
    "            gamma_intermediate[a] = {}\n",
    "            for o in observations:\n",
    "    #             gamma_action_obs = compute_gamma_action_obs(a, o)\n",
    "\n",
    "                gamma_action_obs = []\n",
    "                for alpha in alpha_vecs:\n",
    "                    v = np.zeros(num_states)  # initialize the update vector [0, ... 0]\n",
    "                    for i, si in enumerate(states):\n",
    "                        for j, sj in enumerate(states):\n",
    "                            v[i] += T[a, si, sj] * O[a, sj, o] * alpha.v[j]\n",
    "                        v[i] *= discount\n",
    "                    gamma_action_obs.append(v)\n",
    "\n",
    "                gamma_intermediate[a][o] = gamma_action_obs\n",
    "\n",
    "        # Now compute the cross sum\n",
    "        gamma_action_belief = {}\n",
    "        for a in actions:\n",
    "\n",
    "            gamma_action_belief[a] = {}\n",
    "            for bidx, b in enumerate(belief_points):\n",
    "\n",
    "                gamma_action_belief[a][bidx] = gamma_reward[a].copy()\n",
    "    #             print(\"gamma_reward[a].copy()\", gamma_reward[a].copy())\n",
    "\n",
    "                for o in observations:\n",
    "                    # only consider the best point\n",
    "                    best_alpha_idx = np.argmax(np.dot(gamma_intermediate[a][o], b))\n",
    "    #                 print(\"gamma_intermediate[a][o][best_alpha_idx]\", gamma_intermediate[a][o][best_alpha_idx])\n",
    "                    gamma_action_belief[a][bidx] += gamma_intermediate[a][o][best_alpha_idx]\n",
    "\n",
    "\n",
    "        # Finally compute the new(best) alpha vector set\n",
    "        alpha_vecs, max_val = [], MIN\n",
    "\n",
    "        for bidx, b in enumerate(belief_points):\n",
    "            best_av, best_aa = None, None\n",
    "\n",
    "            for a in actions:\n",
    "                val = np.dot(gamma_action_belief[a][bidx], b)\n",
    "                if best_av is None or val > max_val:\n",
    "                    max_val = val\n",
    "                    best_av = gamma_action_belief[a][bidx].copy()\n",
    "                    best_aa = a\n",
    "\n",
    "            alpha_vecs.append(AlphaVector(a=best_aa, v=best_av))\n",
    "\n",
    "\n",
    "    return alpha_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup alpha vectors\n",
    "alpha_vecs = [AlphaVector(a=-1, v=np.zeros(num_states))] \n",
    "stepsize = 0.1\n",
    "belief_points = [[np.random.uniform() for s in states] for p in np.arange(0., 1. + stepsize, stepsize)]\n",
    "\n",
    "# gamma_reward_og = {\n",
    "#             a: np.frombuffer(array('d', [R[s,a] for s in states]))\n",
    "#             for a in actions\n",
    "#         }\n",
    "\n",
    "gamma_reward = {\n",
    "            a: np.array( [R[a,s] for s in states])\n",
    "            for a in actions\n",
    "        }\n",
    "\n",
    "max_timesteps = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# belief = ctx.random_beliefs() if params.random_prior else ctx.generate_beliefs()\n",
    "rand_nums = np.random.randint(0, 100, size=num_states)\n",
    "base = sum(rand_nums)*1.0\n",
    "belief = [x/base for x in rand_nums]\n",
    "belief = [0.5, 0.5]\n",
    "state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.85, 0.15]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.9697986577181208, 0.0302013422818792]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.85, 0.15000000000000002]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.85, 0.15]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.15, 0.85]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.0302013422818792, 0.9697986577181208]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.005465587044534414, 0.9945344129554656]\n",
      "state  tiger_right\n",
      "action  open_left\n",
      "reward  10.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_left\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.85, 0.15]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.15, 0.85]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.0302013422818792, 0.9697986577181208]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.005465587044534414, 0.9945344129554656]\n",
      "state  tiger_right\n",
      "action  open_left\n",
      "reward  10.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_left\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.15, 0.85]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.0302013422818792, 0.9697986577181208]\n",
      "state  tiger_right\n",
      "action  open_left\n",
      "reward  10.0\n",
      "next_obs  growl_left\n",
      "next_state  tiger_right\n",
      "\n",
      "belief [0.5, 0.5]\n",
      "state  tiger_right\n",
      "action  listen\n",
      "reward  -1.0\n",
      "next_obs  growl_right\n",
      "next_state  tiger_right\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "total_rewards = 0\n",
    "budget = 20\n",
    "max_play = 20\n",
    "for i in range(max_play):\n",
    "    # plan, take action and receive environment feedbacks\n",
    "    alpha_vecs = solve_pomdp(max_timesteps, alpha_vecs, gamma_reward) \n",
    "    print(\"belief\", belief)\n",
    "#     action = pomdp.get_action(belief)\n",
    "    \n",
    "#     print(\"ALPHA VECS\")\n",
    "    for av in alpha_vecs:\n",
    "        v = np.dot(av.v, belief)\n",
    "#         print(\"action: \", av.action)\n",
    "#         print(\"v: \", v)\n",
    "    \n",
    "    \n",
    "    max_v = -np.inf\n",
    "    best = None\n",
    "    for av in alpha_vecs:\n",
    "        v = np.dot(av.v, belief)\n",
    "        if v > max_v:\n",
    "            max_v = v\n",
    "            best = av\n",
    "    action = best.action\n",
    "    \n",
    "    print(\"state \", states_text[state])\n",
    "    print(\"action \", actions_text[action])\n",
    "    \n",
    "    \n",
    "    ai = action\n",
    "    si = state\n",
    "    # get new state\n",
    "    s_probs = [T[ai, si, sj] for sj in states]\n",
    "    next_state = states[np.random.choice(np.arange(num_states), p=s_probs)]\n",
    "\n",
    "    # get new observation\n",
    "    o_probs = [O[ai, next_state, oj] for oj in observations]\n",
    "    next_obs = observations[np.random.choice(np.arange(num_observations), p=o_probs)]\n",
    "\n",
    "    next_reward = R[ai, si]  \n",
    "    next_cost = C[ai]\n",
    "    \n",
    "    print(\"reward \", next_reward)\n",
    "    print(\"next_obs \", observations_text[next_obs])\n",
    "    print(\"next_state \", states_text[next_state])\n",
    "    print()\n",
    "        \n",
    "    new_state, obs, reward, cost = next_state, next_obs, next_reward, next_cost\n",
    "#     state, observation, reward, cost = self.simulate_action(curr_state, action)\n",
    "#     curr_state = state\n",
    "        \n",
    "#     new_state, obs, reward, cost = pomdp.take_action(action)\n",
    "\n",
    "    # update states\n",
    "#     belief = pomdp.update_belief(belief, action, obs)\n",
    "\n",
    "    b_new = []\n",
    "    for sj in states:\n",
    "        p_o_prime = O[action, sj, obs]\n",
    "        summation = 0.0\n",
    "        for i, si in enumerate(states):\n",
    "            p_s_prime = T[action, si, sj]\n",
    "            summation += p_s_prime * float(belief[i])\n",
    "        b_new.append(p_o_prime * summation)\n",
    "\n",
    "    # normalize\n",
    "    total = sum(b_new)\n",
    "    belief = [x / total for x in b_new]\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_rewards += reward\n",
    "    budget -= cost\n",
    "\n",
    "    if budget <= 0:\n",
    "        print('Budget spent.')\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(params.max_play):\n",
    "    # plan, take action and receive environment feedbacks\n",
    "    pomdp.solve(T)\n",
    "    action = pomdp.get_action(belief)\n",
    "    new_state, obs, reward, cost = pomdp.take_action(action)\n",
    "\n",
    "    if params.snapshot and isinstance(pomdp, POMCP):\n",
    "        # takes snapshot of belief tree before it gets updated\n",
    "        self.snapshot_tree(visualiser, pomdp.tree, '{}.gv'.format(i))\n",
    "\n",
    "    # update states\n",
    "    belief = pomdp.update_belief(belief, action, obs)\n",
    "    total_rewards += reward\n",
    "    budget -= cost\n",
    "\n",
    "    if budget <= 0:\n",
    "        log.info('Budget spent.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PBVI():\n",
    "    def __init__(self, model):\n",
    "        self.belief_points = None\n",
    "        self.alpha_vecs = None\n",
    "        self.solved = False\n",
    "        self.model = model\n",
    "\n",
    "    def add_configs(self, belief_points):\n",
    "        self.alpha_vecs = [AlphaVector(a=-1, v=np.zeros(self.model.num_states))] # filled with a dummy alpha vector\n",
    "        self.belief_points = belief_points\n",
    "        self.compute_gamma_reward()\n",
    "\n",
    "    def compute_gamma_reward(self):\n",
    "        \"\"\"\n",
    "        :return: Action_a => Reward(s,a) matrix\n",
    "        \"\"\"\n",
    "\n",
    "        self.gamma_reward = {\n",
    "            a: np.frombuffer(array('d', [self.model.reward_function(a, s) for s in self.model.states]))\n",
    "            for a in self.model.actions\n",
    "        }\n",
    "\n",
    "    def compute_gamma_action_obs(self, a, o):\n",
    "        \"\"\"\n",
    "        Computes a set of vectors, one for each previous alpha\n",
    "        vector that represents the update to that alpha vector\n",
    "        given an action and observation\n",
    "\n",
    "        :param a: action index\n",
    "        :param o: observation index\n",
    "        \"\"\"\n",
    "        m = self.model\n",
    "\n",
    "        gamma_action_obs = []\n",
    "        for alpha in self.alpha_vecs:\n",
    "            v = np.zeros(m.num_states)  # initialize the update vector [0, ... 0]\n",
    "            for i, si in enumerate(m.states):\n",
    "                for j, sj in enumerate(m.states):\n",
    "                    v[i] += m.transition_function(a, si, sj) * \\\n",
    "                        m.observation_function(a, sj, o) * \\\n",
    "                        alpha.v[j]\n",
    "                v[i] *= m.discount\n",
    "            gamma_action_obs.append(v)\n",
    "        return gamma_action_obs\n",
    "\n",
    "    def solve(self, T):\n",
    "        if self.solved:\n",
    "            return\n",
    "\n",
    "        m = self.model\n",
    "        for step in range(T):\n",
    "\n",
    "            # First compute a set of updated vectors for every action/observation pair\n",
    "            # Action(a) => Observation(o) => UpdateOfAlphaVector (a, o)\n",
    "            gamma_intermediate = {\n",
    "                a: {\n",
    "                    o: self.compute_gamma_action_obs(a, o)\n",
    "                    for o in m.observations\n",
    "                } for a in m.actions\n",
    "            }\n",
    "\n",
    "            # Now compute the cross sum\n",
    "            gamma_action_belief = {}\n",
    "            for a in m.actions:\n",
    "\n",
    "                gamma_action_belief[a] = {}\n",
    "                for bidx, b in enumerate(self.belief_points):\n",
    "\n",
    "                    gamma_action_belief[a][bidx] = self.gamma_reward[a].copy()\n",
    "\n",
    "                    for o in m.observations:\n",
    "                        # only consider the best point\n",
    "                        best_alpha_idx = np.argmax(np.dot(gamma_intermediate[a][o], b))\n",
    "                        gamma_action_belief[a][bidx] += gamma_intermediate[a][o][best_alpha_idx]\n",
    "\n",
    "            # Finally compute the new(best) alpha vector set\n",
    "            self.alpha_vecs, max_val = [], MIN\n",
    "\n",
    "            for bidx, b in enumerate(self.belief_points):\n",
    "                best_av, best_aa = None, None\n",
    "\n",
    "                for a in m.actions:\n",
    "                    val = np.dot(gamma_action_belief[a][bidx], b)\n",
    "                    if best_av is None or val > max_val:\n",
    "                        max_val = val\n",
    "                        best_av = gamma_action_belief[a][bidx].copy()\n",
    "                        best_aa = a\n",
    "\n",
    "                self.alpha_vecs.append(AlphaVector(a=best_aa, v=best_av))\n",
    "\n",
    "        self.solved = True\n",
    "\n",
    "    def get_action(self, belief):\n",
    "        max_v = -np.inf\n",
    "        best = None\n",
    "        for av in self.alpha_vecs:\n",
    "            v = np.dot(av.v, belief)\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "                best = av\n",
    "\n",
    "        return best.action\n",
    "    \n",
    "    def update_belief(self, belief, action, obs):\n",
    "        m = self.model\n",
    "\n",
    "        b_new = []\n",
    "        for sj in m.states:\n",
    "            p_o_prime = m.observation_function(action, sj, obs)\n",
    "            summation = 0.0\n",
    "            for i, si in enumerate(m.states):\n",
    "                p_s_prime = m.transition_function(action, si, sj)\n",
    "                summation += p_s_prime * float(belief[i])\n",
    "            b_new.append(p_o_prime * summation)\n",
    "\n",
    "        # normalize\n",
    "        total = sum(b_new)\n",
    "        return [x / total for x in b_new]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
