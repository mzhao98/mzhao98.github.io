{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### An Intuitive Introduction\n",
    "\n",
    "A Markov decision process (MDP) is a stochastic, discrete-time control process. In reinforcement learning (RL), an MDP serves as a mathematical framework for modeling the decision-making of an agent in some environment. Intuitively and perhaps oversimply, we can think of MDPs as modeling some decision-maker (agent), who has a problem to solve (operating properly in some problem-solving environment). Defining an MDP sets up the scenario: agent with the problem to solve. And solving an MDP entails coming up with the \"correct\" set of actions the agent must take to solve the problem. We'll next discuss the MDP definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Definition\n",
    "\n",
    "An MDP is defined by a tuple of variables $\\langle S,A,T,R,\\gamma,P_0 \\rangle$. \n",
    "- $S$ represents the set of states. In other words, this is all of the possible forms the agent's environment could take.\n",
    "- $A$ represents the set of actions available for the agent to take.\n",
    "- $P_0: S \\rightarrow \\mathcal{R}$ represents the initial state distribution. $P_0(s)$ for any state $s \\in S$ represents how likely the agent's environment is to start in state $s$.\n",
    "- $T: S \\times A \\times S \\rightarrow \\mathcal{R}$ is the transition function. $T(s'|s,a)$ gives the probability that the environment transitions to state $s'$ from state $s$ after taking action $a$. \n",
    "- $R: S \\times A \\rightarrow \\mathcal{R}$ represents the agent's objective. The agent's objective is embedded into the environment through the reward. $R(s,a)$ returns a real valued scalar representing how much reward the agent receives for taking action $a$ in state $s$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP Definition: Toy Example\n",
    "\n",
    "The MDP definition is a bit abstract. Let's ground the components of an MDP in a very simple toy example.\n",
    "\n",
    "In our toy example, Alice (our agent) has a problem she needs to solve. The problem is that she dropped both a dirty shirt on the ground in the middle of her room. She needs to quickly and efficiently pick up the dirty shirt and place it in the laundry basket.\n",
    "\n",
    "We'll represent her room as a 5 by 5 grid. On the upper right side, she has a laundry basket. On the lower right side, she has a dresser. Alice is starting at the lower right side, by the door. The dirty shirt is in the middle of the room.\n",
    "\n",
    "An MDP is defined by a tuple of variables $\\langle S,A,T,R,\\gamma,P_0 \\rangle$. \n",
    "- $S$ represents the set of states. These are all of the positions that Alice can be in. All of the coordinate positions in the 5x5 room.\n",
    "- $A$: Alice can [move up, down, left, right, pick up shirt, place shirt].\n",
    "- $P_0$: With probability 1, Alice will begin in the lower right corner. This is deterministic.\n",
    "- $T$: This is also deterministic. All of Alice's action will succeed, if the action she is trying to perform is valid.\n",
    "- $R$: Alice will receive position reward if she places the shirt in the laundry bin. She will receive negative reward if she places the shirt in the dresser, as this will stink up the rest of her clothes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make this even more concrete, and get to coding up this toy example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to import helpful libraries: Numpy for computation and Matplotlib for visualization of states in the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a few global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRTY_SHIRT = 'dirty_shirt'\n",
    "PICKUP = 'pickup'\n",
    "PLACE = 'place'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
